<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <meta name="author" content="Pybonacci">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width">
        <title>Introducción a Machine Learning con Python (Parte 1) | Pybonacci</title>

	<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
	<link rel="icon" href="/favicon.ico" type="image/x-icon">
        <link rel="alternate" type="application/atom+xml" title="Pybonacci blog atom feed" href="/feeds/all.atom.xml" />
        <link href='https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700' rel='stylesheet' type='text/css'>

        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
        <link rel="stylesheet" type="text/css" href="/theme/css/fontello.css"/>
        <style>.highlight .hll { background-color: #ffffcc }
.highlight .c { color: #60a0b0; font-style: italic } /* Comment */
.highlight .err { border: 1px solid #FF0000 } /* Error */
.highlight .k { color: #007020; font-weight: bold } /* Keyword */
.highlight .o { color: #666666 } /* Operator */
.highlight .cm { color: #60a0b0; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #007020 } /* Comment.Preproc */
.highlight .c1 { color: #60a0b0; font-style: italic } /* Comment.Single */
.highlight .cs { color: #60a0b0; background-color: #fff0f0 } /* Comment.Special */
.highlight .gd { color: #A00000 } /* Generic.Deleted */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .gr { color: #FF0000 } /* Generic.Error */
.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight .gi { color: #00A000 } /* Generic.Inserted */
.highlight .go { color: #808080 } /* Generic.Output */
.highlight .gp { color: #c65d09; font-weight: bold } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight .gt { color: #0040D0 } /* Generic.Traceback */
.highlight .kc { color: #007020; font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: #007020; font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: #007020; font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: #007020 } /* Keyword.Pseudo */
.highlight .kr { color: #007020; font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #902000 } /* Keyword.Type */
.highlight .m { color: #40a070 } /* Literal.Number */
.highlight .s { color: #4070a0 } /* Literal.String */
.highlight .na { color: #4070a0 } /* Name.Attribute */
.highlight .nb { color: #007020 } /* Name.Builtin */
.highlight .nc { color: #0e84b5; font-weight: bold } /* Name.Class */
.highlight .no { color: #60add5 } /* Name.Constant */
.highlight .nd { color: #555555; font-weight: bold } /* Name.Decorator */
.highlight .ni { color: #d55537; font-weight: bold } /* Name.Entity */
.highlight .ne { color: #007020 } /* Name.Exception */
.highlight .nf { color: #06287e } /* Name.Function */
.highlight .nl { color: #002070; font-weight: bold } /* Name.Label */
.highlight .nn { color: #0e84b5; font-weight: bold } /* Name.Namespace */
.highlight .nt { color: #062873; font-weight: bold } /* Name.Tag */
.highlight .nv { color: #bb60d5 } /* Name.Variable */
.highlight .ow { color: #007020; font-weight: bold } /* Operator.Word */
.highlight .w { color: #bbbbbb } /* Text.Whitespace */
.highlight .mf { color: #40a070 } /* Literal.Number.Float */
.highlight .mh { color: #40a070 } /* Literal.Number.Hex */
.highlight .mi { color: #40a070 } /* Literal.Number.Integer */
.highlight .mo { color: #40a070 } /* Literal.Number.Oct */
.highlight .sb { color: #4070a0 } /* Literal.String.Backtick */
.highlight .sc { color: #4070a0 } /* Literal.String.Char */
.highlight .sd { color: #4070a0; font-style: italic } /* Literal.String.Doc */
.highlight .s2 { color: #4070a0 } /* Literal.String.Double */
.highlight .se { color: #4070a0; font-weight: bold } /* Literal.String.Escape */
.highlight .sh { color: #4070a0 } /* Literal.String.Heredoc */
.highlight .si { color: #70a0d0; font-style: italic } /* Literal.String.Interpol */
.highlight .sx { color: #c65d09 } /* Literal.String.Other */
.highlight .sr { color: #235388 } /* Literal.String.Regex */
.highlight .s1 { color: #4070a0 } /* Literal.String.Single */
.highlight .ss { color: #517918 } /* Literal.String.Symbol */
.highlight .bp { color: #007020 } /* Name.Builtin.Pseudo */
.highlight .vc { color: #bb60d5 } /* Name.Variable.Class */
.highlight .vg { color: #bb60d5 } /* Name.Variable.Global */
.highlight .vi { color: #bb60d5 } /* Name.Variable.Instance */
.highlight .il { color: #40a070 } /* Literal.Number.Integer.Long */</style>
        <style>.description-author {
  font-size: 0.8em;
  color: #333;
}
.img-author {
  max-height: 10em;
  max-width: 10em;
}
img[src$="centerme"] {
  display: block;
  margin: 0 auto;
}
body {
  margin: 0;
  padding: 0;
  font: 'Source Sans Pro', sans-serif;
  color: #222222;
  text-rendering: optimizeLegibility;
  -webkit-font-smoothing: antialiased;
}
@media all and (min-width: 960px) {
  body {
    font-size: 20px;
  }
}
@media all and (max-width: 959px) and (min-width: 600px) {
  body {
    font-size: 16px;
  }
}
@media all and (max-width: 599px) and (min-width: 320px) {
  body {
    font-size: 14px;
  }
}
div.input_area {
  font-size: 1.1em;
  margin: 0 10%;
}
div.output_area {
  font-size: 1.1em;
  margin: 0 10%;
}
div.output_subarea {
  max-width: 100% !important;
}
pre {
  font-size: 1.1em;
}
p {
  margin-bottom: 20px;
  line-height: 1.6em;
  text-align: justify;
}
img {
  display: block;
  margin-left: auto;
  margin-right: auto;
  max-width: 100%;
}
article {
  margin: 0;
}
article header.about {
  margin-bottom: 0px;
  padding-bottom: 0px;
}
article header {
  padding-bottom: 20px;
}
article header h1 {
  margin-bottom: 2px;
  font-weight: 700;
  color: #000;
}
article header time {
  color: #9E9E9E;
  float: right;
}
article header time.left {
  color: #9E9E9E;
  float: left;
}
article div.social-links ul {
  padding: 0px;
}
article div.social-links li {
  display: inline;
  font-size: 20px;
}
article div.social-links li a {
  color: #000;
  padding: 10px;
}
article div.social-links li a:hover {
  color: #666;
  text-decoration: none;
}
article p.note {
  background: #f5f5f5;
  border: 1px solid #ddd;
  padding: 0.533em 0.733em;
}
article p.update {
  background-color: #FEEFB3;
  border: 1px solid #e6e68a;
  padding: 0.533em 0.733em;
}
article p.alert {
  background-color: #ffe2e2;
  border: 1px solid #ffb2b2;
  padding: 0.533em 0.733em;
}
article a:hover {
  text-decoration: underline;
}
article blockquote {
  border-left: 2px solid #c7c7cc;
  color: #666;
  margin: 30px 0;
  padding: 0 0 0 25px;
}
article .meta {
  margin-top: 35px;
}
article .meta a:hover {
  text-decoration: none;
}
article .meta address:before,
article .meta time:before,
article .meta a.tag:before {
  font-family: 'fontello';
  margin-right: 6px;
}
article .meta address.author {
  float: left;
}
article .meta address:before {
  content: '\e819';
}
article .meta time:before {
  content: '\f133';
}
article .meta div.tags {
  clear: both;
}
article .meta a.tag {
  margin: 0 10px 10px 0;
  padding: 1px 12px;
  display: inline-block;
  font-size: 14px;
  color: rgba(0, 0, 0, 0.8);
  background: rgba(0, 0, 0, 0.05);
}
article .meta a.tag:before {
  content: '\e821';
}
article .meta a.tag:hover {
  background: rgba(0, 0, 0, 0.15);
}
article .meta a.read_more,
article .meta a.comments_btn {
  font-size: 1.5em;
  font-weight: 800;
  padding: 10px 20px;
  color: #007aa3;
  background: #FFF;
  border: 1px solid #007aa3;
}
article .meta a.read_more:hover,
article .meta a.comments_btn:hover {
  color: #FFF;
  background: #007aa3;
}
article .meta:after {
  content: "";
  display: table;
  clear: both;
}</style>

	<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>
	<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>

        <script data-isso="https://comments.pybonacci.org"
                data-isso-lang="es"
                src="https://comments.pybonacci.org/js/embed.min.js"></script>

    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
                processEscapes: true,
                skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
            }
        });
        MathJax.Hub.Queue(function() {
            var all = MathJax.Hub.getAllJax(), i;
            for(i=0; i < all.length; i += 1) {
                all[i].SourceElement().parentNode.className += ' has-jax';
            }
        });
    </script>

    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>


        </script>

    </head>

    <body>
        <header class="navbar navbar-default bs-docs-nav">
            <div class="container-fluid">
                <div class="navbar-header">
		  <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#theNavbar">
		    <span class="icon-bar"></span>
		    <span class="icon-bar"></span>
		    <span class="icon-bar"></span> 
		  </button>
                  <a class="navbar-brand" href="/" title="Home" class="title">Pybonacci</a><!-- — Python y Ciencia-->
                </div>
                <nav class="collapse navbar-collapse bs-navbar-collapse" role="navigation" id="theNavbar">
		    <ul class="nav navbar-nav navbar-right">
                            <li><a href="/pages/acerca-de-pybonacci.html" title="About">Acerca de</a></li>
			    <li><a href="/pages/como-contribuir.html" title="Contributing">Contribuir</a></li>
                            <li><a href="/archives.html" title="Archive">Archivos</a></li>
                            <li><a class="nodec icon-rss" href="/feeds/all.atom.xml" title="pybonacci.github.io RSS feed" rel="me"></a></li>
                    </ul>
                </nav>
            </div>
        </header>

        <div id="wrap">
<div class="container post">
    <article>
        <header>
            <h1>Introducción a Machine Learning con Python (Parte 1)</h1>
            <div class="meta">
                <time datetime="article.date.isoformat()" pubdate>mié 14 enero 2015</time>
                <address class="vcard author">Por
                    <a class="url fn" href="http://pybonacci.github.io/author/pablo-fernandez.html">Pablo Fernández</a>
                </address>
            </div>
        </header>

        <div class="article_content">
            <p>Desde que escuché hablar de Kaggle por primera vez, precisamente <a href="https://twitter.com/Pybonacci/status/414742882679934977" target="_blank">a través de Pybonacci</a>, me entró curiosidad por eso del <em>data science</em> y me propuse como un reto el participar en una de sus competiciones. Para aquel que no la conozca todavía, <a title="Kaggle" href="http://www.kaggle.com/" target="_blank">Kaggle</a> es una plataforma que aloja competiciones de análisis de datos y modelado predictivo donde compañías e investigadores aportan sus datos mientras que estadistas e ingenieros de datos de todo el mundo compiten por crear los mejores modelos de predicción o clasificación.</p>
<p>Muchas y muy diferentes técnicas se pueden aplicar al procesado de datos para generar predicciones, estimaciones o clasificaciones. Desde técnicas de <a href="http://es.wikipedia.org/wiki/Regresión_logística" target="_blank">regresión logística</a> hasta <a href="http://es.wikipedia.org/wiki/Red_neuronal_artificial" target="_blank">redes neuronales artificiales</a> pasando por <a href="http://es.wikipedia.org/wiki/Red_bayesiana" target="_blank">redes bayesianas</a>, <a href="http://es.wikipedia.org/wiki/Máquinas_de_vectores_de_soporte" target="_blank">máquinas de vectores de soporte</a> o <a href="http://es.wikipedia.org/wiki/Árbol_de_decisión" target="_blank">árboles de decisión</a>, en Kaggle no descartan ningún método, e incluso se fomenta la cooperación entre personas con experiencia en diferentes campos para obtener el mejor modelo posible. Varias de estas técnicas se encuadran dentro de lo que es el Machine Learning, o aprendizaje automático, que nos explica <a href="http://en.wikipedia.org/wiki/Jeremy_Howard_(entrepreneur)" target="_blank">Jeremy Howard</a> en el siguiente vídeo.</p>
<!--more Sigue leyendo... >-->

<p>En resumen, el objetivo del machine learning (ML) es enseñar a las máquinas el llevar a cabo ciertas tareas enseñándoles algunos ejemplos de cómo o cómo no llevar a cabo la tarea. Esto rara vez es un proceso en cascada y, en multitud de ocasiones, habrá que retroceder varios pasos para probar diferentes estrategias sobre el conjunto de datos con diferentes algoritmos ML. En palabras de Richert y Coelho (2013), es éste carácter exploratorio lo que se ajusta a la perfección a Python.</p>
<blockquote>
<p>Being an interpreted high-level programming language, it may seem that Python was designed specifically for the process of trying out different things. What is more, it does this very fast.</p>
</blockquote>
<p>Visto el potencial de Python en este campo, la comunidad de desarrolladores ha aportado varios paquetes como <a href="http://pybrain.org/pages/home" target="_blank">PyBrain</a> (Schaul et al., 2010) o <a href="http://scikit-learn.org/stable/index.html" target="_blank">scikit-learn</a> (Pedregosa et al., 2011) entre otros al campo del aprendizaje automático. De todos ellos, el más conocido tal vez sea scikit-learn, y es el que utilizaremos más a menudo para ilustrar los ejemplos.</p>
<p>Pero antes de meternos de lleno al aprendizaje automático vamos a ver unos ejercicios de clasificación básicos que nos permitirán entender mejor la posibilidades y limitaciones de algoritmos mucho más complejos.</p>
<h2>El <em>Iris flower dataset</em></h2>
<p>El conjunto de datos de la planta Iris data de los años '30 y es empleado con frecuencia como ejemplo por diferentes librerías que trabajan con datos o gráficos como <a href="http://pandas.pydata.org/pandas-docs/stable/visualization.html#andrews-curves" target="_blank">pandas</a> o el propio <a href="http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html" target="_blank">scikit-learn</a>. De cada planta de la especie Iris (setosa, versicolor y virginica) se han tomado medidas de longitud y ancho de sépalo y pétalo. Y la pregunta que se suele plantear es: <em>si vemos una nueva planta en el campo, ¿podríamos predecir correctamente su especie a partir de sus medidas?</em></p>
<p>Este es un problema de aprendizaje supervisado o clasificación. Puesto que el conjunto de datos es pequeño, hemos representado las proyecciones bidimensionales como subgráficos de un solo gráfico con el que podemos identificar dos grandes grupos: uno formado por Iris Setosa y otro formado por una mezcla de Iris Versicolor e Iris Virginica.</p>
<h3>Primer modelo de clasificación</h3>
<p>Nuestro primer modelo de clasificación se basará precisamente en esa primera agrupación visual que hemos realizado. Es decir, si la longitud del pétalo es inferior a 2, entonces se trata de Iris Setosa, si no, puede ser Iris Versicolor o Iris Virginica.</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="c1"># leemos el dataset</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="c1"># El dataset contiene 4 atributos</span>
<span class="k">print</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>
<span class="c1"># Separamos Iris Setosa de las otras dos especies en función de la longitud</span>
<span class="c1"># del pétalo (tercer atributo).</span>
<span class="k">for</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">]:</span>
    <span class="k">if</span> <span class="n">value</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">&#39;Iris setosa&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">&#39;Iris virginica o Iris versicolor&#39;</span><span class="p">)</span>
</pre></div>


<p>Lo que hemos creado es un simple umbral en una de las dimensiones. Lo hemos hecho de manera visual; el aprendizaje automático tiene lugar cuando escribimos código que realiza esto mismo por nosotros.</p>
<p>Distinguir Iris Setosa de las otras dos especies fue sencillo. Sin embargo, no tenemos forma de ver inmediatamente cuál es el mejor umbral para distinguir Iris Virginica de Iris Versicolor. Es más, podemos deducir que la distinción nunca será perfecta. Pero podemos generar un algoritmo sencillo que nos de la mejor solución de compromiso en base a los parámetros medidos de las plantas.</p>
<p>En el siguiente fragmento de código vamos a buscar, de entre las cuatro características medidas —longitud y ancho de sépalo y pétalo—, el valor de umbral que mejor clasifica la familia de Iris.</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="c1"># leemos el dataset y pasamos los datos a una DataFrame de pandas por comodidad</span>
<span class="n">sk_iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">sk_iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">sk_iris</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>
<span class="n">iris</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Categorical</span><span class="o">.</span><span class="n">from_codes</span><span class="p">(</span><span class="n">sk_iris</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">sk_iris</span><span class="o">.</span><span class="n">target_names</span><span class="p">)</span>
<span class="c1"># descartamos la familia setosa que ya tenemos clasificada</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">iris</span><span class="p">[</span><span class="n">iris</span><span class="o">.</span><span class="n">labels</span> <span class="o">!=</span> <span class="s1">&#39;setosa&#39;</span><span class="p">]</span>
<span class="n">virginica</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">labels</span><span class="o">==</span><span class="s1">&#39;virginica&#39;</span>
<span class="c1"># obtenemos un array con los nombres de las características que medimos</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">columns</span><span class="p">[:</span><span class="mi">4</span><span class="p">]</span>
<span class="c1"># inicializamos en valor de precisión</span>
<span class="n">best_acc</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="k">for</span> <span class="n">fi</span> <span class="ow">in</span> <span class="n">features</span><span class="p">:</span>                    <span class="c1"># Por cada parámetro o característica de la que tenemos valores</span>
    <span class="n">thresh</span> <span class="o">=</span> <span class="n">iris</span><span class="p">[</span><span class="n">fi</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>           <span class="c1"># obtenemos una lista de valores para el umbral</span>
    <span class="n">thresh</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>   <span class="c1"># que ordenamos de menor a mayor.</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">thresh</span><span class="p">:</span>                   <span class="c1"># Por cada posible valor de umbral</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="p">(</span><span class="n">iris</span><span class="p">[</span><span class="n">fi</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">t</span><span class="p">)</span>          <span class="c1"># determinamos los elementos de la tabla que están por encima</span>
        <span class="n">acc</span> <span class="o">=</span> <span class="p">(</span><span class="n">pred</span><span class="o">==</span><span class="n">virginica</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="c1"># y calculamos que porcentaje de la familia virginica está recogida.</span>
        <span class="k">if</span> <span class="n">acc</span> <span class="o">&gt;</span> <span class="n">best_acc</span><span class="p">:</span>             <span class="c1"># Si mejoramos la detección, actualizamos los parámetro de la colección.</span>
            <span class="n">best_acc</span> <span class="o">=</span> <span class="n">acc</span>             <span class="c1"># Mejor precisión obtenida.</span>
            <span class="n">best_fi</span> <span class="o">=</span> <span class="n">fi</span>               <span class="c1"># Mejor característica para clasificar las familias.</span>
            <span class="n">best_t</span> <span class="o">=</span> <span class="n">t</span>                 <span class="c1"># Valor óptimo de umbral.</span>

<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Mejor precisión obtenida: {:.1%}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">best_acc</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Mejor característica para clasificar: {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">best_fi</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Valor óptimo de umbral: {} cm&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">best_t</span><span class="p">))</span>
</pre></div>


<p>Según el algoritmo que hemos implementado, el valor óptimo de umbral es de 1.6 cm de ancho de pétalo. Con ese valor, clasificamos correctamente el 94% de las plantas como Virginica. En este tipo de modelos de umbral, la frontera de decisión será siempre paralela a uno de los ejes.</p>
<h3>Validación cruzada</h3>
<p>El modelo, a pesar de su simplicidad, logra un 94% de acierto sobre los datos de entrenamiento. No obstante, se trata de una valoración bastante optimista pues empleamos los propios datos de entrenamiento para evaluar el modelo. Lo que realmente queremos es estimar la habilidad del modelo de generalizar en nuevos casos.</p>
<p>Para determinar las capacidades del modelo, u obtener un modelo más robusto, se suele recurrir a la <a href="http://es.wikipedia.org/wiki/Validación_cruzada" target="_blank">validación cruzada</a>. De esta manera utilizamos parte de los datos de que disponemos para entrenar el modelo —<em>training set</em>—, y el resto de datos para probarlo —<em>test set</em>— (ver imagen inferior).<figure style="width: 526px" class="wp-caption aligncenter"></p>
<p><img src="http://upload.wikimedia.org/wikipedia/commons/f/f2/K-fold_cross_validation.jpg" width="526" height="262" class /><figcaption class="wp-caption-text">Validación cruzada de k=4 iteraciones [<a href="http://es.wikipedia.org/wiki/Validaci%C3%B3n_cruzada#Error_de_la_validaci.C3.B3n_cruzada_de_K_iteraciones" target="_blank">Fuente</a>].</figcaption></figure></p>
<h3>K nearest neighbors</h3>
<p>Un paso más hacia lo que sería <em>machine learning</em> es el método <a href="http://es.wikipedia.org/wiki/Knn" target="_blank"><em>k</em>-nn</a> de clasificación supervisada. En este caso, a la hora de clasificar un nuevo elemento, buscamos en el conjunto de datos de que disponemos al punto, o <em>k</em>-puntos más cercanos, y le asignamos su catogoría.</p>
<p>Si bien podemos <a href="http://machinelearningmastery.com/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch/" target="_blank">implementar el algoritmo a mano en Python</a> —nunca está de más saber cómo funcionan las cosas—, scikit-learn incluye la herramienta <a href="http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html" target="_blank">KNeighborsClassifier</a> que ya realiza todo el trabajo pesado por nosotros. Aquí voy a adaptar ligeramente el <a href="http://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html#example-neighbors-plot-classification-py" target="_blank">ejemplo</a> que proporciona scikit-learn para utilizarlo con pandas y generar unos gráficos diferentes que nos permitan compararlo con el modelo anterior.</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">ListedColormap</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
</pre></div>


<p>Vamos a importar ahora el dataset que viene con scikit-learn.</p>
<div class="highlight"><pre><span></span><span class="c1"># leemos el dataset y pasamos los datos a una DataFrame de pandas por comodidad</span>
<span class="n">sk_iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">sk_iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">sk_iris</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>
<span class="c1"># que las etiquetas sean de tipo Categorical será importante más adelante a la hora de crear los gráficos</span>
<span class="n">iris</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Categorical</span><span class="o">.</span><span class="n">from_codes</span><span class="p">(</span><span class="n">sk_iris</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">sk_iris</span><span class="o">.</span><span class="n">target_names</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="p">[[</span><span class="s1">&#39;petal length (cm)&#39;</span><span class="p">,</span> <span class="s1">&#39;petal width (cm)&#39;</span><span class="p">]]</span> <span class="c1"># Tomamos el ancho y longitud del pétalo.</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;category&#39;</span><span class="p">)</span>
</pre></div>


<p>Tomaremos un número de vecinos relativamente grande, <em>k</em> = 15. El valor de <em>k</em> depende mucho de los datos de que dispongamos, pero en general, un valor alto mitiga el ruido a costa de diferenciar menos zonas. Definimos también el espesor de la malla y los <em>colormaps</em> del gráfico.</p>
<div class="highlight"><pre><span></span><span class="n">n_neighbors</span> <span class="o">=</span> <span class="mi">15</span>
<span class="n">h</span> <span class="o">=</span> <span class="o">.</span><span class="mo">02</span>          <span class="c1"># step size de la malla</span>
<span class="c1"># Creamos los colormap</span>
<span class="n">cmap_light</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">([</span><span class="s1">&#39;#FFAAAA&#39;</span><span class="p">,</span> <span class="s1">&#39;#AAFFAA&#39;</span><span class="p">,</span> <span class="s1">&#39;#AAAAFF&#39;</span><span class="p">])</span>
<span class="n">cmap_bold</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">([</span><span class="s1">&#39;#FF0000&#39;</span><span class="p">,</span> <span class="s1">&#39;#00FF00&#39;</span><span class="p">,</span> <span class="s1">&#39;#0000FF&#39;</span><span class="p">])</span>
</pre></div>


<p>Por último, nos metemos de lleno al modelo. Éste, en común a la mayoría de modelos en scikit-learn, dispone de una serie de funciones que se ejecutan paso a paso.</p>
<ul>
<li><em>nombre-del-modelo</em>.<strong>fit()</strong></li>
<li><em>nombre-del-modelo</em>.<strong>predict()</strong></li>
<li><em>nombre-del-modelo</em>.<strong>score()</strong></li>
</ul>
<p>Con la función <code>fit()</code> entrenamos el modelo para obtener los parámetros que utilizaremos sobre los datos de test con la función <code>predict()</code>. Finalmente, con <code>score()</code> podremos obtener una estimación de la capacidad de acierto de nuestro modelo sobre los datos de trabajo.</p>
<div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">weights</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;uniform&#39;</span><span class="p">,</span> <span class="s1">&#39;distance&#39;</span><span class="p">]:</span>
    <span class="c1"># Creamos una instancia de Neighbors Classifier y hacemos un fit a partir de los</span>
    <span class="c1"># datos.</span>
    <span class="c1"># Los pesos (weights) determinarán en qué proporción participa cada punto en la</span>
    <span class="c1"># asignación del espacio. De manera uniforme o proporcional a la distancia.</span>
    <span class="n">clf</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">cat</span><span class="o">.</span><span class="n">codes</span><span class="p">)</span>
    <span class="c1"># Creamos una gráfica con las zonas asignadas a cada categoría según el modelo</span>
    <span class="c1"># k-nearest neighborgs. Para ello empleamos el meshgrid de Numpy.</span>
    <span class="c1"># A cada punto del grid o malla le asignamos una categoría según el modelo knn.</span>
    <span class="c1"># La función c_() de Numpy, concatena columnas.</span>
    <span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span>
                         <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span><span class="p">,</span> <span class="n">h</span><span class="p">))</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span>
    <span class="c1"># Ponemos el resultado en un gráfico.</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">pcolormesh</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap_light</span><span class="p">)</span>
    <span class="c1"># Representamos también los datos de entrenamiento.</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="o">.</span><span class="n">cat</span><span class="o">.</span><span class="n">codes</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap_bold</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">xx</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">yy</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;3-Class classification (k = </span><span class="si">%i</span><span class="s2">, weights = &#39;</span><span class="si">%s</span><span class="s2">&#39;)&quot;</span>
              <span class="o">%</span> <span class="p">(</span><span class="n">n_neighbors</span><span class="p">,</span> <span class="n">weights</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Petal Width&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Petal Length&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;iris-knn-{}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">weights</span><span class="p">))</span>
</pre></div>


<p>Las figuras que obtenemos son las siguientes.</p>
<div id='gallery-1' class='gallery galleryid-2976 gallery-columns-2 gallery-size-thumbnail'>
  <figure class='gallery-item'>

  <div class='gallery-icon landscape'>
    <a href='https://pybonacci.org/images/2015/01/iris-knn-distance.png'><img width="150" height="150" src="https://pybonacci.org/images/2015/01/iris-knn-distance-150x150.png" class="attachment-thumbnail size-thumbnail" alt="" aria-describedby="gallery-1-3078" /></a>
  </div><figcaption class='wp-caption-text gallery-caption' id='gallery-1-3078'> 3-Class classification (k = 15, weights=&#8217;distance&#8217;) </figcaption></figure><figure class='gallery-item'>

  <div class='gallery-icon landscape'>
    <a href='https://pybonacci.org/images/2015/01/iris-knn-uniform.png'><img width="150" height="150" src="https://pybonacci.org/images/2015/01/iris-knn-uniform-150x150.png" class="attachment-thumbnail size-thumbnail" alt="" aria-describedby="gallery-1-3077" /></a>
  </div><figcaption class='wp-caption-text gallery-caption' id='gallery-1-3077'> 3-Class classification (k = 15, weights=&#8217;uniform&#8217;) </figcaption></figure>
</div>

<p>Como podemos ver, en este caso, las lineas que separan las tres categorías de planta Iris ya no son verticales. El modelo <em>k</em> Nearest Neighborgs que hemos empleado ha considerado que la separación de categorías (para selección de ordenadas y abscisas) es más bien inclinada. Visualmente también podemos apreciar que éste modelo incluye más puntos dentro de la categoría correcta que el modelo simple que hemos estudiado en primer lugar. Si recurrimos a la función <code>score()</code>, obtendremos una puntuación del 96% en el caso de <code>weights='uniform'</code>, y del 98.6% si optamos por <code>weights='distancia'</code>. Cabe recordar que en este caso tampoco se ha recurrido a un <em>cross validation</em> para puntuar el modelo, por lo que esta puntuación puede ser algo optimista.</p>
<h2>Conclusión</h2>
<p>Espero que a esta primera parte le siga al menos una segunda, donde si que espero poder enseñar algo de Machine Learning de verdad. No soy ni mucho menos un aficionado en esto del ML, pues hace bien poco que he empezado, pero como es algo por lo que siempre he sentido un cierto interés, creo que merece la pena el que me embarque en enseñar lo que voy aprendiendo; y ver si así despierto el interés de más gente por el tema, perderle el miedo y no verlo como algo lejano y muy complicado.</p>
<p>Por ahora no ha sido más que una introducción y prácticamente no se ha tocado nada de aprendizaje automático, pero hemos tratado algunos conceptos básicos que son clave asimilar primero para poder desarrollar adecuadamente nuestra intuición en la materia. Recalcar, además, la importancia de la estimación del error del modelo. En la próxima entrada explicaré como funciona Kaggle en ese aspecto, con datos de entrenamiento y de test, con <em>leaderboard</em> público y privado.</p>
<p>Cualquier comentario o sugerencia respecto a la entrada es bien recibida. Críticas también. Palos, no. Os leo.</p>
<h2>Referencias</h2>
<p>Kaggle.com, (2015). <em>Kaggle: The Home of Data Science</em>. [online] Available at: http://kaggle.com [Accessed 10 Jan. 2015].</p>
<p>Pedregosa, F., Varoquaux, G., Gramfort, A. et al. (2011). Scikit-learn: Machine Learning in Python. <em>Journal of Machine Learning Research</em>, 12, pp. 2825–2830.</p>
<p>Richert, W. and Coelho, L. (2013). <em>Building Machine Learning Systems with Python</em>. Birmingham: Packt Publishing.</p>
<p>Schaul, T., Bayer, J., Wierstra, D. et al. (2010). PyBrain. <em>Journal of Machine Learning Research</em>, 11, pp. 743–746.</p>
        </div>

        <div class="meta">
            <div class="tags">
                    <a href="http://pybonacci.github.io/tag/dataset.html" class="tag">dataset</a>
                    <a href="http://pybonacci.github.io/tag/iris.html" class="tag">iris</a>
                    <a href="http://pybonacci.github.io/tag/kaggle.html" class="tag">kaggle</a>
                    <a href="http://pybonacci.github.io/tag/machine-learning.html" class="tag">machine learning</a>
                    <a href="http://pybonacci.github.io/tag/python.html" class="tag">python</a>
                    <a href="http://pybonacci.github.io/tag/scikit-learn.html" class="tag">scikit-learn</a>
            </div>
        </div>


    </article>
<section id="isso-thread">
    <h3>Comentarios</h3>
</section>
</div>

<style type="text/css">
{
    max-width: 700px;
}

.text_cell .prompt {
    display: none;
}

div.cell {
    padding: 0;
}

div.text_cell_render {
    padding: 0;
}

div.prompt {
    font-size: 13px;
}

div.input_prompt {
    padding: .7em 0.2em;
}

div.output_prompt {
    padding: .4em .2em;
}

div.input_area {
}

table.dataframe {
    font-family: Arial, sans-serif;
    font-size: 13px;
    line-height: 20px;
}

table.dataframe th, td {
    padding: 4px;
    text-align: left;
}

pre code {
    background-color: inherit;
}</style>

        </div>

        <footer class="disclaimer">
          <div class="container-fluid">
            <p>
              © 2012-2018 Pybonacci, licencia <a href="https://github.com/Pybonacci/pybonacci.github.io/blob/sources/LICENSE.md"> CC BY-SA 4.0 + MIT</a>
              salvo otra indicación.
              <p>Contenido generado con <a href= "http://docs.getpelican.com/">Pelican</a>.</p>
            </p>
          </div>
        </footer>

    </body>
</html>